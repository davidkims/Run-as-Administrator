name: "üè¶ FinOps + EchoOps v8.1.1 (SimWrite Verified: DB+Kafka+Reports+Vault+Obs+SBOM+Scan)"

on:
  workflow_dispatch:
    inputs:
      rows:
        description: "ÏãúÎÆ¨Î†àÏù¥ÏÖò Í±∞Îûò Í±¥Ïàò"
        required: true
        default: "2000"
      rows_per_commit:
        description: "Î∞∞Ïπò Ïª§Î∞ã ÌÅ¨Í∏∞(ÏßÅÏ†ë DB Î™®Îìú)"
        required: true
        default: "500"
      seed:
        description: "ÎÇúÏàò ÏãúÎìú(ÎπàÍ∞í=ÎûúÎç§)"
        required: false
        default: ""
      strict_writes:
        description: "Ïì∞Í∏∞ Í≤ÄÏ¶ù Î∂àÏùºÏπòÏãú Ïã§Ìå®(true/false)"
        required: true
        default: "true"
      parquet_outputs:
        description: "CSV Ïô∏ Parquet ÏÇ∞Ï∂úÎ¨º ÏÉùÏÑ±(true/false)"
        required: true
        default: "true"
      run_kafka:
        description: "Kafka/ZK + Ïä§Ìä∏Î¶¨Î∞ç ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ (true/false)"
        required: true
        default: "false"
      extra_accounts:
        description: "Ï∂îÍ∞Ä Í∞ÄÏÉÅÍ≥ÑÏ¢å Ïàò(Í∏∞Î≥∏ 50 + N)"
        required: true
        default: "150"
      encrypt_outputs:
        description: "ÏÇ∞Ï∂úÎ¨º AES-256 ÏïîÌò∏Ìôî(zip.enc) (true/false)"
        required: true
        default: "true"
      enc_pass:
        description: "ÏïîÌò∏Ìôî ÎπÑÎ∞ÄÎ≤àÌò∏(ÏÑ†ÌÉù, ÎØ∏ÏßÄÏ†ïÏãú ÎûúÎç§)"
        required: false
        default: ""
      commit_changes:
        description: "ÏóêÏΩî ÏÇ∞Ï∂úÎ¨º Ïª§Î∞ã/Ìë∏Ïãú (true/false)"
        required: true
        default: "true"
      create_release:
        description: "GitHub Release ÏÉùÏÑ± (true/false)"
        required: true
        default: "false"
      enable_teradata:
        description: "Teradata Ïª®ÌÖåÏù¥ÎÑà ÏãúÎèÑ (true/false)"
        required: true
        default: "false"
      teradata_token:
        description: "Teradata Docker ÌÜ†ÌÅ∞(ÏÑ†ÌÉù)"
        required: false
        default: ""

permissions:
  contents: write
  packages: read

concurrency:
  group: finops-echoops-${{ github.ref }}
  cancel-in-progress: false

jobs:
  finops:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    env:
      TZ: UTC

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Toolchain (Python 3.11)
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Bootstrap Echo Helpers
        shell: bash
        run: |
          set -Eeuo pipefail
          cat > /tmp/echo_helpers.sh <<'SH'
          set -Eeuo pipefail
          TS(){ date +"%Y-%m-%d %H:%M:%S%z"; }
          echoe(){ echo "[$(TS)] [ECHO] $*"; }
          warn(){  echo "[$(TS)] [WARN] $*" >&2; }
          fail(){  echo "[$(TS)] [FAIL] $*" >&2; exit 1; }
          run(){   echoe "$*"; eval "$@"; }
          retry(){ local n=${1:-5}; shift; local s=${1:-3}; shift; local i
                   for i in $(seq 1 "$n"); do
                     if eval "$*"; then return 0; fi
                     warn "retry $i/$n: $*"; sleep "$s"
                   done; return 1; }
          SH
          chmod +x /tmp/echo_helpers.sh

      - name: Echo + Massive Directories (+RUN_ID)
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          echoe "ÎîîÎ†âÌÜ†Î¶¨/Î©îÌÉÄ ÏÉùÏÑ±"
          mkdir -p \
            .github/echo/{logs,sql,bin,configs,imgs,reports,tmp,crypto,mass,monitoring} \
            .github/echo_db/{ddl,dml,seed,views,backup} \
            .github/echo_td/{sim,docs,logs} \
            site/{reports,imgs,logs,sql,docs} \
            artifacts/{imgs,reports,sql,logs,zip,db,sbom,scan}
          for d in $(seq 1 20); do mkdir -p ".github/echo/mass/dir_$d/sub_$d"; done
          RID="$(uuidgen 2>/dev/null || cat /proc/sys/kernel/random/uuid)"
          echo "$RID" > .github/echo/logs/run.id
          echo "RUN_ID=$RID" >> "$GITHUB_ENV"
          echo "RUN_LABEL=echo-$(date -u +%Y%m%dT%H%M%SZ)-$RID" >> "$GITHUB_ENV"
          printf "%s\n" "started=$(date -u +%FT%TZ)" "actor=$GITHUB_ACTOR" "rows=${{ inputs.rows }}" "run_id=$RID" > .github/echo/logs/run.meta

      - name: Install/Upgrade Docker (official repo)
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          run 'docker --version || true'
          run 'sudo apt-get update -y'
          run 'sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release jq unzip zip openssl'
          run 'curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --yes --dearmor -o /usr/share/keyrings/docker.gpg'
          echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo $VERSION_CODENAME) stable" \
            | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
          run 'sudo apt-get update -y'
          run 'sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin'
          run 'docker --version'
          run 'docker compose version || true'

      - name: Start Core DB & Tools (Postgres + Adminer + MinIO)
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          NET="finops_net"
          run "docker network inspect '$NET' >/dev/null 2>&1 || docker network create '$NET'"
          run "docker rm -f finops-db finops-adminer finops-minio >/dev/null 2>&1 || true"
          run "docker run -d --name finops-db --network '$NET' -e POSTGRES_PASSWORD=finops -e POSTGRES_USER=finops -e POSTGRES_DB=finops -p 5432:5432 postgres:16"
          run "docker run -d --name finops-adminer --network '$NET' -e ADMINER_DEFAULT_SERVER=finops-db -p 8080:8080 adminer:4"
          run "docker run -d --name finops-minio --network '$NET' -e MINIO_ROOT_USER=admin -e MINIO_ROOT_PASSWORD=admin12345 -p 9000:9000 -p 9001:9001 minio/minio server /data --console-address ':9001'"
          run "docker ps --format 'table {{.Names}}\t{{.Image}}\t{{.Status}}'"

      - name: Observability Stack (Prometheus + Grafana + Loki + Promtail + cAdvisor + node-exporter)
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          NET="finops_net"
          mkdir -p .github/echo/monitoring
          cat > .github/echo/monitoring/prometheus.yml <<'YML'
          global:
            scrape_interval: 15s
          scrape_configs:
            - job_name: 'prometheus'
              static_configs: [{ targets: ['prometheus:9090'] }]
            - job_name: 'node'
              static_configs: [{ targets: ['node-exporter:9100'] }]
            - job_name: 'cadvisor'
              static_configs: [{ targets: ['cadvisor:8080'] }]
          YML
          cat > .github/echo/monitoring/promtail-config.yml <<'YML'
          server:
            http_listen_port: 9080
            grpc_listen_port: 0
          positions:
            filename: /tmp/positions.yaml
          clients:
            - url: http://loki:3100/loki/api/v1/push
          scrape_configs:
            - job_name: docker-logs
              static_configs:
                - targets: [localhost]
                  labels:
                    job: docker
                    __path__: /var/lib/docker/containers/*/*-json.log
          YML
          run "docker rm -f prometheus grafana loki promtail cadvisor node-exporter >/dev/null 2>&1 || true"
          run "docker run -d --name prometheus --network '$NET' -p 9090:9090 -v '$PWD/.github/echo/monitoring/prometheus.yml':/etc/prometheus/prometheus.yml prom/prometheus"
          run "docker run -d --name grafana --network '$NET' -p 3000:3000 -e GF_SECURITY_ADMIN_PASSWORD=admin grafana/grafana"
          run "docker run -d --name loki --network '$NET' -p 3100:3100 grafana/loki:2.9.4"
          run "docker run -d --name promtail --network '$NET' -v /var/lib/docker/containers:/var/lib/docker/containers:ro -v '$PWD/.github/echo/monitoring/promtail-config.yml':/etc/promtail/config.yml -p 9080:9080 grafana/promtail:2.9.4 -config.file=/etc/promtail/config.yml"
          run "docker run -d --name cadvisor --network '$NET' -p 8081:8080 --privileged -v /:/rootfs:ro -v /var/run:/var/run:ro -v /sys:/sys:ro -v /var/lib/docker/:/var/lib/docker:ro gcr.io/cadvisor/cadvisor:v0.47.2"
          run "docker run -d --name node-exporter --network '$NET' -p 9100:9100 quay.io/prometheus/node-exporter"

      - name: (Optional) Pre-write enc_pass from input
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          if [ -n "${{ inputs.enc_pass }}" ]; then
            echoe "enc_pass ÌîÑÎ¶¨ÏãúÎìú"
            printf "enc_pass=%s\n" "${{ inputs.enc_pass }}" > .github/echo/crypto/enc.pass
          fi

      - name: Vault (dev) ‚Äì robust pull + fallback + store enc_pass
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          NET="finops_net"
          IMG_PRIMARY="hashicorp/vault:1.17"
          IMG_FALLBACK_1="hashicorp/vault:1.16"
          IMG_FALLBACK_2="hashicorp/vault:latest"
          CNTR="finops-vault"
          docker rm -f "$CNTR" >/dev/null 2>&1 || true
          run "docker network inspect '$NET' >/dev/null 2>&1 || docker network create '$NET'"
          pull_try(){ retry 4 4 "docker pull $1"; }
          if ! pull_try "$IMG_PRIMARY"; then
            warn "fallback to $IMG_FALLBACK_1"; if ! pull_try "$IMG_FALLBACK_1"; then
              warn "fallback to $IMG_FALLBACK_2"; pull_try "$IMG_FALLBACK_2"; fi; fi
          if docker image inspect "$IMG_PRIMARY" >/dev/null 2>&1; then USE_IMG="$IMG_PRIMARY";
          elif docker image inspect "$IMG_FALLBACK_1" >/dev/null 2>&1; then USE_IMG="$IMG_FALLBACK_1"; else USE_IMG="$IMG_FALLBACK_2"; fi
          echoe "using image: $USE_IMG"
          run "docker run -d --name '$CNTR' --network '$NET' -p 8200:8200 -e VAULT_DEV_ROOT_TOKEN_ID=root -e VAULT_ADDR=http://0.0.0.0:8200 '$USE_IMG'"
          for i in $(seq 1 20); do
            if docker exec "$CNTR" sh -lc "apk add --no-cache curl >/dev/null 2>&1 || (apt-get update -y >/dev/null 2>&1 && apt-get install -y curl >/dev/null 2>&1 || true); curl -s http://127.0.0.1:8200/v1/sys/health >/dev/null 2>&1"; then echoe "Vault ready"; break; fi
            echoe "waiting vault...($i)"; sleep 2; done
          PASS_FILE=".github/echo/crypto/enc.pass"
          if [ -f "$PASS_FILE" ]; then
            ENC="$(sed -n 's/^enc_pass=//p' "$PASS_FILE" | head -n1)"
            docker exec "$CNTR" sh -lc "curl -s --header 'X-Vault-Token: root' --request POST --data '{\"data\":{\"enc_pass\":\"${ENC}\"}}' http://127.0.0.1:8200/v1/secret/data/finops_enc >/dev/null 2>&1 && echo '[ECHO] enc_pass stored in dev Vault'"
          fi

      - name: (Optional) Kafka + ZooKeeper + Topics
        if: ${{ inputs.run_kafka == 'true' }}
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          NET="finops_net"
          run "docker rm -f finops-zk finops-kafka >/dev/null 2>&1 || true"
          run "docker run -d --name finops-zk --network '$NET' -p 2181:2181 -e ALLOW_ANONYMOUS_LOGIN=yes bitnami/zookeeper:3.9"
          run "docker run -d --name finops-kafka --network '$NET' -p 9092:9092 \
               -e KAFKA_BROKER_ID=1 \
               -e KAFKA_CFG_ZOOKEEPER_CONNECT=finops-zk:2181 \
               -e ALLOW_PLAINTEXT_LISTENER=yes \
               -e KAFKA_CFG_LISTENERS=PLAINTEXT://:9092 \
               -e KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \
               bitnami/kafka:3.7"
          retry 20 3 "docker exec finops-kafka /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list >/dev/null 2>&1"
          run "docker exec finops-kafka /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --if-not-exists --topic transactions --replication-factor 1 --partitions 3"

      - name: Wait for Postgres & Install DB clients
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          for i in $(seq 1 40); do
            if docker exec finops-db pg_isready -U finops -d finops >/dev/null 2>&1; then echoe "Postgres ready"; break; fi
            echoe "waiting postgres...($i)"; sleep 3; done
          run "sudo apt-get update -y"
          run "sudo apt-get install -y postgresql-client"

      - name: Generate DDL/Views (with RUN tables/columns)
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          cat > .github/echo_db/ddl/000_extensions.sql <<'SQL'
          create extension if not exists pgcrypto;
          SQL
          cat > .github/echo_db/ddl/001_core.sql <<'SQL'
          create table if not exists accounts (
            account_id bigserial primary key,
            account_no varchar(32) unique not null,
            holder_name varchar(128) not null,
            opened_at timestamptz not null default now(),
            status varchar(16) not null default 'ACTIVE'
          );
          create table if not exists journal_entries (
            je_id bigserial primary key,
            ts timestamptz not null default now(),
            account_no varchar(32) not null,
            type varchar(8) not null check (type in ('DEBIT','CREDIT')),
            amount numeric(18,2) not null check (amount>=0),
            currency varchar(8) not null default 'KRW',
            memo text,
            run_id uuid
          );
          create index if not exists ix_journal_account_ts on journal_entries(account_no, ts);
          create index if not exists ix_journal_run on journal_entries(run_id);
          create table if not exists transactions (
            txn_id bigserial primary key,
            ts timestamptz not null default now(),
            account_no varchar(32) not null,
            direction varchar(8) not null check (direction in ('IN','OUT')),
            amount numeric(18,2) not null check (amount>=0),
            balance_after numeric(18,2) not null,
            ref varchar(64),
            note text,
            run_id uuid
          );
          create index if not exists ix_txn_account_ts on transactions(account_no, ts);
          create index if not exists ix_txn_run on transactions(run_id);
          create table if not exists balances (
            account_no varchar(32) primary key,
            balance numeric(18,2) not null default 0
          );
          SQL
          cat > .github/echo_db/ddl/002_sim.sql <<'SQL'
          create table if not exists sim_runs (
            run_id uuid primary key,
            started_at timestamptz not null default now(),
            finished_at timestamptz,
            actor text,
            mode text,
            rows_target int not null,
            rows_written int default 0,
            status text default 'RUNNING',
            notes text
          );
          SQL
          cat > .github/echo_db/views/100_views.sql <<'SQL'
          create or replace view v_income_statement as
          select date_trunc('day', ts) as d,
                 sum(case when direction='IN'  then amount else 0 end) as revenue,
                 sum(case when direction='OUT' then amount else 0 end) as expense,
                 sum(case when direction='IN'  then amount else -amount end) as profit
          from transactions
          group by 1
          order by 1;
          create or replace view v_balance_sheet as
          select sum(balance) as total_assets
          from balances;
          create or replace view td_sim_daily as
          select date_trunc('day', ts) as d,
                 count(*) as txn_cnt,
                 sum(amount) filter (where direction='IN') as sum_in,
                 sum(amount) filter (where direction='OUT') as sum_out
          from transactions
          group by 1
          order by 1;
          SQL

      - name: Apply Schema (PostgreSQL)
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -f .github/echo_db/ddl/000_extensions.sql"
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -f .github/echo_db/ddl/001_core.sql"
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -f .github/echo_db/ddl/002_sim.sql"
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -f .github/echo_db/views/100_views.sql"

      - name: Seed Accounts (50 + extra) via generate_series
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          ACC=$((50 + ${{ inputs.extra_accounts }}))
          cat > .github/echo_db/seed/accounts.sql <<'SQL'
          \set ACC :ACC
          with s as (select generate_series(1, :ACC) as i)
          insert into accounts(account_no, holder_name)
          select 'V' || to_char(i,'FM000000'), 'User ' || i from s
          on conflict (account_no) do nothing;
          with s as (select generate_series(1, :ACC) as i)
          insert into balances(account_no, balance)
          select 'V' || to_char(i,'FM000000'), 0 from s
          on conflict (account_no) do nothing;
          SQL
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -v ACC=\"$ACC\" -f .github/echo_db/seed/accounts.sql"

      - name: Write Simulators (Kafka mode) ‚Äì includes RUN_ID
        if: ${{ inputs.run_kafka == 'true' }}
        shell: bash
        env:
          RUN_ID: ${{ env.RUN_ID }}
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          echo "kafka-python" > .github/echo/requirements-extra.txt
          cat > .github/echo/bin/kafka_producer.py <<'PY'
          import os, json, random
          from datetime import datetime, timedelta
          from kafka import KafkaProducer
          ROWS=int(os.getenv("ROWS","2000"))
          RUN_ID=os.getenv("RUN_ID")
          EXTRA=int(os.getenv("EXTRA","150"))
          producer=KafkaProducer(bootstrap_servers="localhost:9092", value_serializer=lambda v: json.dumps(v, default=str).encode())
          accts=[f"V{str(i).zfill(6)}" for i in range(1, 50+EXTRA+1)]
          seed=os.getenv("SEED")
          if seed: random.seed(int(seed))
          base=datetime.utcnow()-timedelta(days=3)
          for i in range(ROWS):
            msg={
              "run_id": RUN_ID,
              "ts": (base+timedelta(seconds=i*(3*24*3600/ROWS))).isoformat(),
              "account_no": random.choice(accts),
              "direction": random.choice(["IN","OUT"]),
              "amount": round(random.uniform(10, 500000),2),
              "ref": f"KAFKA{i}"
            }
            producer.send("transactions", msg)
            if i%500==0: producer.flush()
          producer.flush()
          PY
          cat > .github/echo/bin/kafka_consumer.py <<'PY'
          import json, psycopg2
          from kafka import KafkaConsumer
          consumer=KafkaConsumer("transactions", bootstrap_servers="localhost:9092", value_deserializer=lambda m: json.loads(m.decode()))
          conn=psycopg2.connect(host="127.0.0.1", user="finops", password="finops", dbname="finops")
          cur=conn.cursor()
          for msg in consumer:
            r=msg.value
            rid=r.get("run_id")
            acc=r["account_no"]; direction=r["direction"]; amount=float(r["amount"]); ts=r["ts"]
            cur.execute("select balance from balances where account_no=%s",(acc,))
            row=cur.fetchone(); bal=float(row[0]) if row else 0.0
            new_bal=bal+amount if direction=="IN" else max(0.0, bal-amount)
            cur.execute("""insert into transactions(account_no,direction,amount,balance_after,ref,note,ts,run_id)
                           values(%s,%s,%s,%s,%s,%s,%s,%s)""",(acc,direction,amount,new_bal,r.get("ref","kafka"),"kafka",ts,rid))
            jtype="DEBIT" if direction=="IN" else "CREDIT"
            cur.execute("""insert into journal_entries(account_no,type,amount,ts,memo,run_id)
                           values(%s,%s,%s,%s,%s,%s)""",(acc,jtype,amount,ts,"kafka",rid))
            cur.execute("""insert into balances(account_no,balance)
                           values(%s,%s)
                           on conflict (account_no) do update set balance=excluded.balance""",(acc,new_bal))
            conn.commit()
          PY

      - name: Write Simulator (direct DB mode, batched) ‚Äì includes RUN_ID
        if: ${{ inputs.run_kafka != 'true' }}
        shell: bash
        env:
          RUN_ID: ${{ env.RUN_ID }}
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          cat > .github/echo/requirements.txt <<'PIP'
          psycopg2-binary
          matplotlib
          pyarrow
          PIP
          cat > .github/echo/bin/simulate.py <<'PY'
          import os, random, csv, json
          from datetime import datetime, timedelta
          import psycopg2, psycopg2.extras
          import matplotlib.pyplot as plt
          ROWS=int(os.getenv("ROWS","2000"))
          RUN_ID=os.getenv("RUN_ID")
          ROWS_PER_COMMIT=int(os.getenv("ROWS_PER_COMMIT","500"))
          SEED=os.getenv("SEED")
          if SEED: random.seed(int(SEED))
          conn=psycopg2.connect(host="127.0.0.1",user="finops",password="finops",dbname="finops")
          cur=conn.cursor()
          # run Î©îÌÉÄ ÏãúÏûë
          MODE="direct-db"
          ACTOR=os.getenv("GITHUB_ACTOR","unknown")
          cur.execute("insert into sim_runs(run_id, actor, mode, rows_target, status, notes) values(%s,%s,%s,%s,'RUNNING',%s) on conflict (run_id) do nothing",
                      (RUN_ID, ACTOR, MODE, ROWS, "started"))
          conn.commit()
          # Í≥ÑÏ¢å Ï°∞Ìöå
          cur.execute("select account_no from accounts order by account_no;")
          accts=[r[0] for r in cur.fetchall()]
          base=datetime.utcnow()-timedelta(days=10)
          rows=[]
          tx_batch=[]
          je_batch=[]
          for i in range(ROWS):
            acc=random.choice(accts)
            ts=base+timedelta(seconds=i*(10*24*3600/ROWS))
            direction=random.choice(["IN","OUT"])
            amount=round(random.uniform(10, 800000),2)
            # ÏûîÏï° Ï°∞Ìöå
            cur.execute("select balance from balances where account_no=%s",(acc,))
            row=cur.fetchone(); bal=float(row[0]) if row else 0.0
            new_bal=bal+amount if direction=="IN" else max(0.0, bal-amount)
            # Î∞∞Ïπò Í∏∞Î°ù
            tx_batch.append((ts,acc,direction,amount,new_bal,f"REF{i}","sim",RUN_ID))
            jtype="DEBIT" if direction=="IN" else "CREDIT"
            je_batch.append((acc,jtype,amount,ts,"sim",RUN_ID))
            rows.append((ts,acc,direction,amount,new_bal))
            # Ï¶âÏãú ÏûîÏï° ÏóÖÏÑúÌä∏
            cur.execute("""insert into balances(account_no,balance)
                           values(%s,%s)
                           on conflict (account_no) do update set balance=excluded.balance""",(acc,new_bal))
            if len(tx_batch)>=ROWS_PER_COMMIT:
              psycopg2.extras.execute_values(cur,
                """insert into transactions(ts,account_no,direction,amount,balance_after,ref,note,run_id)
                   values %s""", tx_batch)
              psycopg2.extras.execute_values(cur,
                """insert into journal_entries(account_no,type,amount,ts,memo,run_id)
                   values %s""", je_batch)
              conn.commit()
              tx_batch.clear(); je_batch.clear()
          if tx_batch:
            psycopg2.extras.execute_values(cur,
              """insert into transactions(ts,account_no,direction,amount,balance_after,ref,note,run_id)
                 values %s""", tx_batch)
            psycopg2.extras.execute_values(cur,
              """insert into journal_entries(account_no,type,amount,ts,memo,run_id)
                 values %s""", je_batch)
            conn.commit()
          # charts
          cur.execute("select date_trunc('day', ts) as d, sum(case when direction='IN' then amount else 0 end) as rev, \
                       sum(case when direction='OUT' then amount else 0 end) as exp, \
                       sum(case when direction='IN' then amount else -amount end) as prof \
                       from transactions where run_id=%s group by 1 order by 1;", (RUN_ID,))
          D=[(r[0],float(r[1] or 0),float(r[2] or 0),float(r[3] or 0)) for r in cur.fetchall()]
          dates=[r[0] for r in D]; rev=[r[1] for r in D]; exp=[r[2] for r in D]; prof=[r[3] for r in D]
          import os
          os.makedirs("site/imgs",exist_ok=True)
          plt.figure(); plt.plot(dates,rev,label="Revenue"); plt.plot(dates,exp,label="Expense"); plt.plot(dates,prof,label="Profit"); plt.legend(); plt.tight_layout()
          plt.savefig("site/imgs/income_statement.png")
          cur.execute("select sum(balance) from balances;"); assets=float(cur.fetchone()[0] or 0)
          plt.figure(); plt.bar(["Assets"],[assets]); plt.tight_layout(); plt.savefig("site/imgs/balance_sheet.png")
          # CSV/Parquet
          os.makedirs("site/reports",exist_ok=True)
          with open("site/reports/transactions.csv","w",newline="",encoding="utf-8") as f:
            w=csv.writer(f); w.writerow(["ts","account_no","direction","amount","balance_after"]); w.writerows(rows)
          if os.getenv("PARQUET","true").lower()=="true":
            try:
              import pyarrow as pa, pyarrow.parquet as pq
              table=pa.Table.from_pylist([{"ts":r[0],"account_no":r[1],"direction":r[2],"amount":r[3],"balance_after":r[4]} for r in rows])
              pq.write_table(table,"site/reports/transactions.parquet")
            except Exception as e:
              print("parquet write skipped:", e)
          # run Î©îÌÉÄ Ï¢ÖÎ£å Ï†Ñ Í∞ØÏàò ÌôïÏù∏
          cur.execute("select count(*) from transactions where run_id=%s",(RUN_ID,)); txn_count=cur.fetchone()[0]
          cur.execute("update sim_runs set rows_written=%s, finished_at=now(), status='DONE' where run_id=%s",(txn_count, RUN_ID))
          conn.commit(); cur.close(); conn.close()
          # Î¶¨Ìè¨Ìä∏ JSON
          report={"run_id":RUN_ID,"mode":MODE,"rows_target":ROWS,"rows_written":int(txn_count)}
          with open("site/reports/write_report.json","w",encoding="utf-8") as f: json.dump(report,f,ensure_ascii=False,indent=2)
          PY

      - name: Install Python deps & Run simulators
        shell: bash
        env:
          ROWS: ${{ inputs.rows }}
          ROWS_PER_COMMIT: ${{ inputs.rows_per_commit }}
          SEED: ${{ inputs.seed }}
          PARQUET: ${{ inputs.parquet_outputs }}
          EXTRA: ${{ inputs.extra_accounts }}
          RUN_ID: ${{ env.RUN_ID }}
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          run "python -m pip install --upgrade pip"
          echo "psycopg2-binary" > .github/echo/requirements-common.txt
          echo "matplotlib" >> .github/echo/requirements-common.txt
          run "pip install -r .github/echo/requirements-common.txt || true"
          if [ "${{ inputs.run_kafka }}" = "true" ]; then
            run "pip install -r .github/echo/requirements-extra.txt"
            run "python .github/echo/bin/kafka_consumer.py >/tmp/kafka_consumer.log 2>&1 & echo \$! > /tmp/kafka_consumer.pid"
            run "EXTRA='${{ inputs.extra_accounts }}' ROWS='${{ inputs.rows }}' SEED='${{ inputs.seed }}' RUN_ID='${RUN_ID}' python .github/echo/bin/kafka_producer.py"
            sleep 5; cp /tmp/kafka_consumer.log .github/echo/logs/kafka_consumer.log || true
          else
            run "pip install -r .github/echo/requirements.txt"
            run "ROWS_PER_COMMIT='${{ inputs.rows_per_commit }}' SEED='${{ inputs.seed }}' RUN_ID='${RUN_ID}' python .github/echo/bin/simulate.py"
          fi

      - name: Verify Simulation Writes (poll until match)
        shell: bash
        env:
          RUN_ID: ${{ env.RUN_ID }}
          ROWS: ${{ inputs.rows }}
          STRICT: ${{ inputs.strict_writes }}
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          cat > .github/echo/bin/verify.py <<'PY'
          import os, time, json, psycopg2
          RUN_ID=os.getenv("RUN_ID"); TARGET=int(os.getenv("ROWS","0"))
          STRICT=os.getenv("STRICT","true").lower()=="true"
          conn=psycopg2.connect(host="127.0.0.1", user="finops", password="finops", dbname="finops")
          cur=conn.cursor()
          ok=False
          for _ in range(60):  # ÏµúÎåÄ 60Ï¥à Ìè¥ÎßÅ (Kafka ÏÜåÎπÑ Î≥¥Ï†ï)
            cur.execute("select count(*) from transactions where run_id=%s",(RUN_ID,)); c1=cur.fetchone()[0]
            cur.execute("select count(*) from journal_entries where run_id=%s",(RUN_ID,)); c2=cur.fetchone()[0]
            if c1==TARGET and c2==TARGET:
              ok=True; break
            time.sleep(1)
          cur.execute("select min(ts), max(ts) from transactions where run_id=%s",(RUN_ID,)); tmin,tmax=cur.fetchone()
          report={"run_id":RUN_ID,"target":TARGET,"txn_count":int(c1),"journal_count":int(c2),"time_range":[str(tmin),str(tmax)],"strict":STRICT,"ok":ok}
          os.makedirs("site/reports",exist_ok=True)
          with open("site/reports/write_verify.json","w") as f: json.dump(report,f,indent=2)
          cur.close(); conn.close()
          if STRICT and not ok:
            raise SystemExit("STRICT write verification failed: target=%d, txn=%d, journal=%d"%(TARGET,c1,c2))
          PY
          python .github/echo/bin/verify.py

          # Kafka consumer Ï¢ÖÎ£å(ÏûàÎã§Î©¥)
          if [ -f /tmp/kafka_consumer.pid ]; then
            kill "$(cat /tmp/kafka_consumer.pid)" 2>/dev/null || true
          fi

      - name: MinIO Client (mc) & Upload results to bucket
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          run "curl -fsSL https://dl.min.io/client/mc/release/linux-amd64/mc -o mc && chmod +x mc"
          retry 20 2 "./mc alias set local http://127.0.0.1:9000 admin admin12345"
          run "./mc mb --ignore-existing local/finops"
          run "./mc mirror --overwrite site local/finops/site"
          run "./mc mirror --overwrite .github/echo local/finops/echo"
          run "./mc ls -r local/finops | tee .github/echo/logs/minio.tree"

      - name: DB Backup (pg_dumpall)
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          run "PGPASSWORD=finops pg_dumpall -h 127.0.0.1 -U finops > .github/echo_db/backup/pg_dumpall.sql"
          run "gzip -c .github/echo_db/backup/pg_dumpall.sql > artifacts/db/pg_dumpall.sql.gz"

      - name: Package outputs
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          run "mkdir -p artifacts/zip"
          run "cp -a .github/echo_db/ddl site/sql/"
          run "cp -a .github/echo_db/seed site/sql/ || true"
          run "cp -a .github/echo_db/views site/sql/"
          run "cp -a site/imgs artifacts/imgs/"
          run "cp -a site/reports artifacts/reports/"
          run "cp -a site/sql artifacts/sql/"
          run "cp -a .github/echo/logs artifacts/logs/"
          run "cp -a .github/echo_db/backup artifacts/db/"
          (cd artifacts && zip -r zip/finops_outputs.zip imgs reports sql logs db >/dev/null)

      - name: SBOM (syft)
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          SYFT_VER="v1.31.0"
          URL_PRIMARY="https://raw.githubusercontent.com/anchore/syft/main/install.sh"
          URL_FALLBACK="https://raw.githubusercontent.com/anchore/syft/${SYFT_VER}/install.sh"
          if ! curl -fsSL "$URL_PRIMARY" | bash -s -- -b /usr/local/bin "$SYFT_VER"; then
            warn "primary install script failed ‚Üí try tag-specific"
            curl -fsSL "$URL_FALLBACK" | bash -s -- -b /usr/local/bin "$SYFT_VER"
          fi
          run "syft version"
          run "syft dir:. -o spdx-json > artifacts/sbom/repo.spdx.json"

      - name: Security Scan (Trivy + Gitleaks)
        shell: bash
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          # Trivy (config -> misconfig)
          run "curl -fsSL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin"
          run "trivy fs --scanners vuln,misconfig,secret --format json --output artifacts/scan/trivy.json ."
          # Gitleaks ÏµúÏã† Î¶¥Î¶¨Ïä§ ÏûêÏÇ∞ ÌÉêÏÉâ ÏÑ§Ïπò
          API="https://api.github.com/repos/gitleaks/gitleaks/releases/latest"
          ASSET_URL="$(curl -fsSL -H "Authorization: Bearer ${GH_TOKEN}" "$API" \
            | jq -r '.assets[] | select(.name | test("linux_x64.*\\.tar\\.gz$")) | .browser_download_url' | head -n1)"
          if [ -z "$ASSET_URL" ] || [ "$ASSET_URL" = "null" ]; then
            warn "latest asset not found via API, fallback to v8.18.4"
            ASSET_URL="https://github.com/gitleaks/gitleaks/releases/download/v8.18.4/gitleaks_8.18.4_linux_x64.tar.gz"
          fi
          echoe "Gitleaks asset: $ASSET_URL"
          curl -fsSL "$ASSET_URL" -o /tmp/gitleaks.tgz
          tar -xzf /tmp/gitleaks.tgz -C /usr/local/bin gitleaks || \
            (tar -xzf /tmp/gitleaks.tgz -C /tmp && mv /tmp/gitleaks*/gitleaks /usr/local/bin/)
          chmod +x /usr/local/bin/gitleaks
          run "gitleaks version || true"
          run "gitleaks detect --redact -v --no-git -o artifacts/scan/gitleaks.sarif --report-format sarif || true"

      - name: Encrypt outputs (optional AES-256)
        if: ${{ inputs.encrypt_outputs == 'true' }}
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          PASS="${{ inputs.enc_pass }}"
          if [ -z "$PASS" ]; then PASS="$(openssl rand -hex 16)"; fi
          printf "%s\n" "enc_pass=$PASS" > .github/echo/crypto/enc.pass
          run "openssl enc -aes-256-cbc -md sha256 -salt -pbkdf2 -in artifacts/zip/finops_outputs.zip -out artifacts/zip/finops_outputs.zip.enc -k '$PASS'"
          echoe "ÏïîÌò∏Ìôî ÏôÑÎ£å ‚Üí artifacts/zip/finops_outputs.zip.enc (ÎπÑÎ≤à: .github/echo/crypto/enc.pass)"

      - name: Strip secrets before upload/commit
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          run "rm -f .github/echo/crypto/enc.pass || true"

      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: finops-echoops-v811
          path: |
            artifacts/**
            site/**
            .github/echo/**
          if-no-files-found: warn
          retention-days: 14

      - name: Commit & Push (safe refspec)
        if: ${{ inputs.commit_changes == 'true' }}
        shell: bash
        env:
          GIT_AUTHOR_NAME: "github-actions[bot]"
          GIT_AUTHOR_EMAIL: "41898282+github-actions[bot]@users.noreply.github.com"
          GIT_COMMITTER_NAME: "github-actions[bot]"
          GIT_COMMITTER_EMAIL: "41898282+github-actions[bot]@users.noreply.github.com"
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          BRANCH="${GITHUB_REF_NAME:-main}"
          run "git config user.name  \"$GIT_AUTHOR_NAME\""
          run "git config user.email \"$GIT_AUTHOR_EMAIL\""
          run "git add -A"
          if git diff --cached --quiet; then echoe "Î≥ÄÍ≤Ω ÏóÜÏùå ‚Üí Ìë∏Ïãú Ïä§ÌÇµ"; exit 0; fi
          run "git commit -m \"[ECHO] v8.1.1 (sim-write verified) rows=${{ inputs.rows }}, kafka=${{ inputs.run_kafka }}, strict=${{ inputs.strict_writes }}\""
          run "git fetch --tags --force origin"
          run "git push origin \"refs/heads/${BRANCH}:refs/heads/${BRANCH}\" --force-with-lease"

      - name: Create Release (optional)
        if: ${{ inputs.create_release == 'true' }}
        uses: softprops/action-gh-release@v2
        with:
          tag_name: finops-echoops-v811-${{ github.run_number }}
          name: "FinOps EchoOps v8.1.1 ‚Ä¢ Run #${{ github.run_number }}"
          draft: false
          prerelease: false
          files: |
            artifacts/zip/finops_outputs.zip
            artifacts/zip/finops_outputs.zip.enc

      # - name: (Optional) Free disk space
      #   shell: bash
      #   run: |
      #     set -Eeuo pipefail
      #     docker system prune -af || true
