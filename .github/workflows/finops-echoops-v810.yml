name: "üè¶ FinOps + EchoOps v8.1.0 ‚Ä¢ XL (HealthGate+PostHook+Summary+Grype+Cosign+Checksums+PipCache+Grafana+OTel+LokiRetention+OptionsJSON+PartitionsCOPY+Formats+Verify+DBTune+PgBouncer)"

on:
  workflow_dispatch:
    inputs:
      rows:
        description: "ÏãúÎÆ¨Î†àÏù¥ÏÖò Í±∞Îûò Í±¥Ïàò"
        required: true
        default: "2000"
      commit_changes:
        description: "ÏóêÏΩî ÏÇ∞Ï∂úÎ¨º Ïª§Î∞ã/Ìë∏Ïãú (true/false)"
        required: true
        default: "true"
      create_release:
        description: "GitHub Release ÏÉùÏÑ± (true/false)"
        required: true
        default: "false"
      enable_teradata:
        description: "Teradata Ïª®ÌÖåÏù¥ÎÑà ÏãúÎèÑ (true/false)"
        required: true
        default: "false"
      teradata_token:
        description: "Teradata Docker ÌÜ†ÌÅ∞(ÏÑ†ÌÉù)"
        required: false
        default: ""
      run_kafka:
        description: "Kafka/ZK + Ïä§Ìä∏Î¶¨Î∞ç ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ (true/false)"
        required: true
        default: "false"
      encrypt_outputs:
        description: "ÏÇ∞Ï∂úÎ¨º AES-256 ÏïîÌò∏Ìôî(zip.enc) (true/false)"
        required: true
        default: "true"
      enc_pass:
        description: "ÏïîÌò∏Ìôî ÎπÑÎ∞ÄÎ≤àÌò∏(ÏÑ†ÌÉù, ÎØ∏ÏßÄÏ†ïÏãú ÎûúÎç§)"
        required: false
        default: ""
      extra_accounts:
        description: "Ï∂îÍ∞Ä Í∞ÄÏÉÅÍ≥ÑÏ¢å Ïàò(Í∏∞Î≥∏ 50 + N)"
        required: true
        default: "150"
      options_json:
        description: "Í≥†Í∏â ÏòµÏÖò(JSON: parquet_outputs, jsonl_outputs, enc_pass, enable_teradata, teradata_token)"
        required: false
        default: ""
      # --- DB Tuning & Partitions ---
      db_tune_mode:
        description: "DB ÌäúÎãù Î™®Îìú(auto/manual)"
        required: true
        default: "auto"
      db_shared_buffers_mb:
        description: "manual Î™®Îìú: shared_buffers (MB)"
        required: false
        default: ""
      db_work_mem_mb:
        description: "manual Î™®Îìú: work_mem (MB)"
        required: false
        default: ""
      db_maintenance_work_mem_mb:
        description: "manual Î™®Îìú: maintenance_work_mem (MB)"
        required: false
        default: ""
      db_effective_cache_size_mb:
        description: "manual Î™®Îìú: effective_cache_size (MB)"
        required: false
        default: ""
      db_max_connections:
        description: "manual Î™®Îìú: max_connections"
        required: false
        default: ""
      db_autovacuum:
        description: "autovacuum (on/off)"
        required: true
        default: "on"
      db_wal_keep_size_mb:
        description: "WAL Î≥¥Ï°¥ ÏÇ¨Ïù¥Ï¶à(MB)"
        required: false
        default: "256"
      part_days_back:
        description: "ÌååÌã∞ÏÖò Í≥ºÍ±∞ ÏùºÏàò(Í∏∞Î≥∏ 60)"
        required: false
        default: "60"
      part_days_forward:
        description: "ÌååÌã∞ÏÖò ÎØ∏Îûò ÏùºÏàò(Í∏∞Î≥∏ 7)"
        required: false
        default: "7"
      enable_pgbouncer:
        description: "PgBouncer Ïó∞Í≤∞ÌíÄÎü¨ ÌôúÏÑ±Ìôî(true/false)"
        required: true
        default: "false"
      pgbouncer_pool_size:
        description: "PgBouncer ÌíÄ ÏÇ¨Ïù¥Ï¶à(per db/user)"
        required: false
        default: "50"

permissions:
  contents: write
  packages: read
  id-token: write   # cosign keyless OIDC

concurrency:
  group: finops-echoops-${{ github.ref }}
  cancel-in-progress: false

jobs:
  finops:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    strategy:
      fail-fast: false
      matrix:
        rows: ["${{ inputs.rows }}"]
        run_kafka: ["${{ inputs.run_kafka }}"]
        encrypt_outputs: ["${{ inputs.encrypt_outputs }}"]
    env:
      ECHO_FETCH_BASE: ""   # ÏõêÍ≤© ÌÖúÌîåÎ¶ø Ïò§Î≤ÑÎùºÏù¥Îìú Î≤†Ïù¥Ïä§ URL(ÎØ∏ÏßÄÏ†ïÏãú Î°úÏª¨ heredoc)
      ECHO_XTRACE: "true"
      TZ: UTC

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Ï†ÑÏó≠ ÏóêÏΩî/Ìó¨Ìçº + ÏûêÎèô Ï£ºÏûÖ
      - name: EchoOps ‚Ä¢ enable global echo & helpers
        shell: bash
        run: |
          set -Eeuo pipefail
          cat > /tmp/echo_helpers.sh <<'SH'
          set -Eeuo pipefail
          TS(){ date +"%Y-%m-%d %H:%M:%S%z"; }
          echoe(){ echo "[$(TS)] [ECHO] $*"; }
          warn(){  echo "[$(TS)] [WARN] $*" >&2; }
          fail(){  echo "[$(TS)] [FAIL] $*" >&2; exit 1; }
          run(){   echoe "$*"; eval "$@"; }
          retry(){ local n=${1:-5}; shift; local s=${1:-3}; shift; local i
                   for i in $(seq 1 "$n"); do
                     if eval "$*"; then return 0; fi
                     warn "retry $i/$n: $*"; sleep "$s"
                   done; return 1; }
          fetch_or_fail(){ # $1=remote_rel_path $2=dest_path
            local rel="$1"; local dest="$2"
            if [ -n "${ECHO_FETCH_BASE:-}" ]; then
              local url="${ECHO_FETCH_BASE%/}/$rel"
              echoe "fetch $url -> $dest"
              curl -fsSL "$url" -o "$dest"
            else
              return 1
            fi
          }
          SH
          chmod +x /tmp/echo_helpers.sh
          cat > /tmp/echo_env <<'ENV'
          set -Eeuo pipefail
          if [ "${ECHO_XTRACE:-true}" = "true" ]; then
            export PS4='+ [$(date "+%Y-%m-%d %H:%M:%S%z")] [TRACE] '
            set -x
          fi
          source /tmp/echo_helpers.sh
          ENV
          echo "BASH_ENV=/tmp/echo_env" >> "$GITHUB_ENV"

      # Python + pip Ï∫êÏãú
      - name: Toolchain (Python 3.11 with pip cache)
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      # ÏòµÏÖò JSON ÌååÏÑú(ÌôòÍ≤ΩÎ≥ÄÏàò Î∞òÏòÅ)
      - name: Parse options_json ‚Üí ENV (echo)
        shell: bash
        env:
          OPTS_JSON: ${{ inputs.options_json }}
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          echoe "parsing options_json..."
          python - <<'PY'
import os, json, sys
opts = os.environ.get("OPTS_JSON","").strip()

cfg = {
  "parquet_outputs": True,
  "jsonl_outputs": False,
  "enc_pass": "",
  "enable_teradata": False,
  "teradata_token": ""
}

if opts:
  try:
    parsed = json.loads(opts)
    if isinstance(parsed, dict):
      cfg.update(parsed)
    else:
      print("[WARN] options_json must be an object (dict). Ignored.", file=sys.stderr)
  except Exception as e:
    print(f"[WARN] options_json parse error: {e}", file=sys.stderr)

with open(os.environ["GITHUB_ENV"], "a", encoding="utf-8") as f:
  f.write(f"PARQUET={'true' if cfg['parquet_outputs'] else 'false'}\n")
  f.write(f"JSONL={'true' if cfg['jsonl_outputs'] else 'false'}\n")
  f.write(f"ENC_PASS_OPT={str(cfg['enc_pass']).strip()}\n")
  f.write(f"ENABLE_TERADATA_OPT={'true' if cfg['enable_teradata'] else 'false'}\n")
  f.write(f"TERADATA_TOKEN_OPT={str(cfg['teradata_token']).strip()}\n")
PY
          echoe "options_json ‚Üí ENV completed"

      - name: Echo + Massive Directories
        shell: bash
        run: |
          run 'mkdir -p .github/echo/{logs,sql,bin,configs,imgs,reports,tmp,crypto,mass,monitoring}'
          run 'mkdir -p .github/echo_db/{ddl,dml,seed,views,backup}'
          run 'mkdir -p .github/echo_td/{sim,docs,logs}'
          run 'mkdir -p site/{reports,imgs,logs,sql,docs}'
          run 'mkdir -p artifacts/{imgs,reports,sql,logs,zip,db,sbom,scan,checks}'
          for d in $(seq 1 20); do run "mkdir -p .github/echo/mass/dir_$d/sub_$d"; done
          echoe "write .github/echo/logs/run.meta"
          printf "%s\n" "started=$(date -u +%FT%TZ)" "actor=$GITHUB_ACTOR" "rows=${{ matrix.rows }}" > .github/echo/logs/run.meta

      - name: Install/Upgrade Docker (official repo)
        shell: bash
        run: |
          run 'docker --version || true'
          run 'sudo apt-get update -y'
          run 'sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release jq unzip zip openssl'
          run 'curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --yes --dearmor -o /usr/share/keyrings/docker.gpg'
          run 'bash -lc "echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo $VERSION_CODENAME) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null"'
          run 'sudo apt-get update -y'
          run 'sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin'
          run 'docker --version'
          run 'docker compose version || true'

      # Core DB/Tools + HealthGate
      - name: Start Core DB & Tools (Postgres + Adminer + MinIO)
        shell: bash
        run: |
          NET="finops_net"
          VOL_PG="finops_pgdata"
          run "docker network inspect '$NET' >/dev/null 2>&1 || docker network create '$NET'"
          run "docker rm -f finops-db finops-adminer finops-minio >/dev/null 2>&1 || true"
          run "docker volume inspect '$VOL_PG' >/dev/null 2>&1 || docker volume create '$VOL_PG' >/dev/null"
          run "docker run -d --name finops-db --network '$NET' \
               -e POSTGRES_PASSWORD=finops -e POSTGRES_USER=finops -e POSTGRES_DB=finops \
               -v '$VOL_PG':/var/lib/postgresql/data -p 5432:5432 postgres:16"
          run "docker run -d --name finops-adminer --network '$NET' -e ADMINER_DEFAULT_SERVER=finops-db -p 8080:8080 adminer:4"
          run "docker run -d --name finops-minio --network '$NET' \
               -e MINIO_ROOT_USER=admin -e MINIO_ROOT_PASSWORD=admin12345 \
               -p 9000:9000 -p 9001:9001 minio/minio server /data --console-address ':9001'"
          retry 30 2 "docker exec finops-db pg_isready -U finops -d finops"
          if ! curl -fsS -o /dev/null http://127.0.0.1:8080; then
            warn "Adminer health failed"; docker logs --tail=200 finops-adminer || true; fail "Adminer not ready"
          fi
          if ! curl -fsS -o /dev/null http://127.0.0.1:9000/minio/health/ready; then
            warn "MinIO health failed"; docker logs --tail=200 finops-minio || true; fail "MinIO not ready"
          fi
          run "docker ps --format 'table {{.Names}}\t{{.Image}}\t{{.Status}}'"

      - name: Observability (Prometheus + Grafana + Loki-retention + Promtail + cAdvisor + node-exporter + OTel)
        shell: bash
        run: |
          NET="finops_net"
          run 'mkdir -p .github/echo/monitoring'
          # prometheus.yml
          if ! fetch_or_fail ".github/echo/monitoring/prometheus.yml" ".github/echo/monitoring/prometheus.yml"; then
            echoe "write .github/echo/monitoring/prometheus.yml"
            cat > .github/echo/monitoring/prometheus.yml <<'YML'
          global: { scrape_interval: 15s }
          rule_files: ["/etc/prometheus/rules/*.yml"]
          scrape_configs:
            - job_name: 'prometheus'
              static_configs: [{ targets: ['prometheus:9090'] }]
            - job_name: 'node'
              static_configs: [{ targets: ['node-exporter:9100'] }]
            - job_name: 'cadvisor'
              static_configs: [{ targets: ['cadvisor:8080'] }]
          YML
          fi
          run 'mkdir -p .github/echo/monitoring/rules'
          cat > .github/echo/monitoring/rules/alerts.yml <<'YML'
          groups:
            - name: echo-alerts
              rules:
                - alert: PostgresDown
                  expr: up{job="prometheus"}==1 and absent(up{job="postgres"})
                  for: 2m
                  labels: { severity: warning }
                  annotations: { summary: "Postgres may be down" }
          YML
          # promtail
          if ! fetch_or_fail ".github/echo/monitoring/promtail-config.yml" ".github/echo/monitoring/promtail-config.yml"; then
            echoe "write .github/echo/monitoring/promtail-config.yml"
            cat > .github/echo/monitoring/promtail-config.yml <<'YML'
          server: { http_listen_port: 9080, grpc_listen_port: 0 }
          positions: { filename: /tmp/positions.yaml }
          clients: [ { url: http://loki:3100/loki/api/v1/push } ]
          scrape_configs:
            - job_name: docker-logs
              static_configs:
                - targets: [localhost]
                  labels: { job: docker, __path__: /var/lib/docker/containers/*/*-json.log }
          YML
          fi
          # otel-collector (mini)
          cat > .github/echo/monitoring/otel-config.yml <<'YML'
          receivers: { otlp: { protocols: { http: { endpoint: "0.0.0.0:4318" } } } }
          exporters: { logging: {} }
          service: { pipelines: { traces: { receivers: [otlp], exporters: [logging] } } }
          YML
          run "docker rm -f prometheus grafana loki promtail cadvisor node-exporter otel >/dev/null 2>&1 || true"
          run "docker run -d --name prometheus --network '$NET' -p 9090:9090 \
               -v '$PWD/.github/echo/monitoring/prometheus.yml':/etc/prometheus/prometheus.yml \
               -v '$PWD/.github/echo/monitoring/rules':/etc/prometheus/rules \
               prom/prometheus"
          run "docker run -d --name grafana --network '$NET' -p 3000:3000 -e GF_SECURITY_ADMIN_PASSWORD=admin grafana/grafana"
          # Loki with retention & compactor args
          run "docker run -d --name loki --network '$NET' -p 3100:3100 \
               grafana/loki:2.9.4 -config.expand-env=true -log.level=warn -distributor.ring.instance-addr=0.0.0.0 \
               -common.compactor.bloom-tenant-index=false -limits_config.retention_period=24h"
          run "docker run -d --name promtail --network '$NET' \
               -v /var/lib/docker/containers:/var/lib/docker/containers:ro \
               -v '$PWD/.github/echo/monitoring/promtail-config.yml':/etc/promtail/config.yml \
               -p 9080:9080 grafana/promtail:2.9.4 -config.file=/etc/promtail/config.yml"
          run "docker run -d --name cadvisor --network '$NET' -p 8081:8080 --privileged \
               -v /:/rootfs:ro -v /var/run:/var/run:ro -v /sys:/sys:ro -v /var/lib/docker/:/var/lib/docker:ro gcr.io/cadvisor/cadvisor:v0.47.2"
          run "docker run -d --name node-exporter --network '$NET' -p 9100:9100 quay.io/prometheus/node-exporter"
          run "docker run -d --name otel --network '$NET' -p 4318:4318 \
               -v '$PWD/.github/echo/monitoring/otel-config.yml':/etc/otelcol/config.yml otel/opentelemetry-collector:0.98.0 \
               --config=/etc/otelcol/config.yml"
          # HealthGate
          retry 30 2 "curl -fsS http://127.0.0.1:9090/-/ready >/dev/null"
          retry 30 2 "curl -fsS http://127.0.0.1:3000/api/health >/dev/null"
          retry 30 2 "curl -fsS http://127.0.0.1:3100/ready >/dev/null"
          retry 30 2 "curl -fsS http://127.0.0.1:9080 >/dev/null"
          retry 30 2 "curl -fsS http://127.0.0.1:8081/containers/ >/dev/null"
          retry 30 2 "curl -fsS http://127.0.0.1:9100/metrics >/dev/null"

      - name: (Optional) Kafka + ZooKeeper + Topics
        if: ${{ matrix.run_kafka == 'true' }}
        shell: bash
        run: |
          NET="finops_net"
          run "docker rm -f finops-zk finops-kafka >/dev/null 2>&1 || true"
          run "docker run -d --name finops-zk --network '$NET' -p 2181:2181 -e ALLOW_ANONYMOUS_LOGIN=yes bitnami/zookeeper:3.9"
          run "docker run -d --name finops-kafka --network '$NET' -p 9092:9092 \
               -e KAFKA_BROKER_ID=1 \
               -e KAFKA_CFG_ZOOKEEPER_CONNECT=finops-zk:2181 \
               -e ALLOW_PLAINTEXT_LISTENER=yes \
               -e KAFKA_CFG_LISTENERS=PLAINTEXT://:9092 \
               -e KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \
               bitnami/kafka:3.7"
          retry 40 3 "docker exec finops-kafka /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list >/dev/null 2>&1" || { docker logs --tail=200 finops-kafka || true; fail "Kafka not ready"; }
          run "docker exec finops-kafka /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --if-not-exists --topic transactions --replication-factor 1 --partitions 3"

      - name: Wait for Postgres & psql client
        shell: bash
        run: |
          retry 40 3 "docker exec finops-db pg_isready -U finops -d finops"
          run "sudo apt-get update -y"
          run "sudo apt-get install -y postgresql-client"
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -c 'select 1'"

      # Vault + enc_pass Í≤∞Ï†ï(ÏûÖÎ†•/options_json/Secrets Ìè¥Î∞±)
      - name: Vault (dev) + enc_pass seed (options/secret fallback)
        shell: bash
        env:
          ENC_PASS_INPUT: ${{ inputs.enc_pass }}
          ENC_PASS_OPT: ${{ env.ENC_PASS_OPT }}
          ENC_PASS_FALLBACK: ${{ secrets.ENC_PASS_FALLBACK }}
        run: |
          PASS="${ENC_PASS_INPUT:-}"
          [ -z "$PASS" ] && PASS="${ENC_PASS_OPT:-}"
          [ -z "$PASS" ] && PASS="${ENC_PASS_FALLBACK:-}"
          if [ -n "$PASS" ]; then
            echoe "write .github/echo/crypto/enc.pass"
            printf "enc_pass=%s\n" "$PASS" > .github/echo/crypto/enc.pass
          else
            warn "enc_pass not provided; will generate later if encryption enabled"
          fi
          NET="finops_net"
          IMG="hashicorp/vault:1.17"
          CNTR="finops-vault"
          run "docker rm -f '$CNTR' >/dev/null 2>&1 || true"
          run "docker network inspect '$NET' >/dev/null 2>&1 || docker network create '$NET'"
          retry 4 4 "docker pull $IMG"
          run "docker run -d --name '$CNTR' --network '$NET' -p 8200:8200 -e VAULT_DEV_ROOT_TOKEN_ID=root -e VAULT_ADDR=http://0.0.0.0:8200 $IMG"
          retry 20 2 "docker exec '$CNTR' sh -lc \"apk add --no-cache curl >/dev/null 2>&1 || (apt-get update -y >/dev/null 2>&1 && apt-get install -y curl >/dev/null 2>&1 || true); curl -fsS http://127.0.0.1:8200/v1/sys/health >/dev/null\""
          if [ -f .github/echo/crypto/enc.pass ]; then
            ENC="$(sed -n 's/^enc_pass=//p' .github/echo/crypto/enc.pass | head -n1)"
            run "docker exec '$CNTR' sh -lc \"curl -s --header 'X-Vault-Token: root' --request POST --data '{\\\"data\\\":{\\\"enc_pass\\\":\\\"${ENC}\\\"}}' http://127.0.0.1:8200/v1/secret/data/finops_enc >/dev/null 2>&1\""
            echoe "enc_pass stored in dev Vault"
          fi

      # DDL: ÌååÌã∞ÏÖò ÌÖåÏù¥Î∏î + Î∑∞
      - name: Generate DDL/Views (Partitions)
        shell: bash
        run: |
          run 'mkdir -p .github/echo_db/{ddl,views}'
          echoe "write .github/echo_db/ddl/001_core.sql"
          cat > .github/echo_db/ddl/001_core.sql <<'SQL'
          create extension if not exists pgcrypto;
          create table if not exists accounts (
            account_id bigserial primary key,
            account_no varchar(32) unique not null,
            holder_name varchar(128) not null,
            opened_at timestamptz not null default now(),
            status varchar(16) not null default 'ACTIVE'
          );
          create table if not exists journal_entries (
            je_id bigserial,
            ts timestamptz not null default now(),
            account_no varchar(32) not null,
            type varchar(8) not null check (type in ('DEBIT','CREDIT')),
            amount numeric(18,2) not null check (amount>=0),
            currency varchar(8) not null default 'KRW',
            memo text,
            run_id uuid
          ) partition by range (ts);
          create table if not exists transactions (
            txn_id bigserial,
            ts timestamptz not null default now(),
            account_no varchar(32) not null,
            direction varchar(8) not null check (direction in ('IN','OUT')),
            amount numeric(18,2) not null check (amount>=0),
            balance_after numeric(18,2) not null,
            ref varchar(64),
            note text,
            run_id uuid
          ) partition by range (ts);
          create table if not exists balances (
            account_no varchar(32) primary key,
            balance numeric(18,2) not null default 0
          );
          SQL
          echoe "write .github/echo_db/ddl/003_partitions.sql"
          cat > .github/echo_db/ddl/003_partitions.sql <<'SQL'
          do $$
          declare d date := (current_date - interval '60 days')::date;
                  endd date := (current_date + interval '7 days')::date;
          begin
            while d <= endd loop
              execute format('create table if not exists transactions_p%s partition of transactions for values from (%L) to (%L);',
                             to_char(d,'YYYYMMDD'), d::timestamptz, (d+1)::timestamptz);
              execute format('create index if not exists ix_txn_%s_acc_ts on transactions_p%s (account_no, ts);',
                             to_char(d,'YYYYMMDD'), to_char(d,'YYYYMMDD'));
              execute format('create table if not exists journal_entries_p%s partition of journal_entries for values from (%L) to (%L);',
                             to_char(d,'YYYYMMDD'), d::timestamptz, (d+1)::timestamptz);
              execute format('create index if not exists ix_je_%s_acc_ts on journal_entries_p%s (account_no, ts);',
                             to_char(d,'YYYYMMDD'), to_char(d,'YYYYMMDD'));
              d := d + 1;
            end loop;
          end$$;
          SQL
          echoe "write .github/echo_db/views/100_views.sql"
          cat > .github/echo_db/views/100_views.sql <<'SQL'
          create or replace view v_income_statement as
          select date_trunc('day', ts) as d,
                 sum(case when direction='IN'  then amount else 0 end) as revenue,
                 sum(case when direction='OUT' then amount else 0 end) as expense,
                 sum(case when direction='IN'  then amount else -amount end) as profit
          from transactions
          group by 1
          order by 1;
          create or replace view v_balance_sheet as
          select sum(balance) as total_assets from balances;
          create or replace view td_sim_daily as
          select date_trunc('day', ts) as d,
                 count(*) as txn_cnt,
                 sum(amount) filter (where direction='IN') as sum_in,
                 sum(amount) filter (where direction='OUT') as sum_out
          from transactions
          group by 1
          order by 1;
          SQL

      - name: Apply Schema (PostgreSQL)
        shell: bash
        run: |
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -f .github/echo_db/ddl/001_core.sql"
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -f .github/echo_db/ddl/003_partitions.sql"
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -f .github/echo_db/views/100_views.sql"

      - name: Seed Accounts
        shell: bash
        run: |
          run 'mkdir -p .github/echo_db/seed'
          echoe "write .github/echo_db/seed/accounts.sql"
          cat > .github/echo_db/seed/accounts.sql <<'SQL'
          \set ACC :ACC
          with s as (select generate_series(1, :ACC) as i)
          insert into accounts(account_no, holder_name)
          select 'V' || to_char(i,'FM000000'), 'User ' || i from s
          on conflict (account_no) do nothing;
          with s as (select generate_series(1, :ACC) as i)
          insert into balances(account_no, balance)
          select 'V' || to_char(i,'FM000000'), 0 from s
          on conflict (account_no) do nothing;
          SQL
          ACC=$((50 + ${{ inputs.extra_accounts }}))
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -v ACC=\"$ACC\" -f .github/echo_db/seed/accounts.sql"

      # === DB ÌäúÎãù(ÏûêÎèô/ÏàòÎèô) ‚Üí ALTER SYSTEM ‚Üí Ïû¨Í∏∞Îèô/Í≤ÄÏ¶ù ===
      - name: DB Tuning Profile ‚Ä¢ generate (auto/manual)
        shell: bash
        env:
          MODE: ${{ inputs.db_tune_mode }}
          SBUFF: ${{ inputs.db_shared_buffers_mb }}
          WMEM: ${{ inputs.db_work_mem_mb }}
          MW_MEM: ${{ inputs.db_maintenance_work_mem_mb }}
          ECACHE: ${{ inputs.db_effective_cache_size_mb }}
          MAXCONN: ${{ inputs.db_max_connections }}
          AUTOVAC: ${{ inputs.db_autovacuum }}
          WALKEEP: ${{ inputs.db_wal_keep_size_mb }}
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          mem_total_kb=$(grep MemTotal /proc/meminfo | awk '{print $2}')
          mem_total_mb=$((mem_total_kb/1024))
          cpu_cnt=$(nproc)

          auto_shared=$(( mem_total_mb * 25 / 100 ))
          (( auto_shared < 256 )) && auto_shared=256
          auto_ecache=$(( mem_total_mb * 60 / 100 ))
          (( auto_ecache < 512 )) && auto_ecache=512
          auto_wmem=$(( (mem_total_mb * 1) / (100 * cpu_cnt) ))
          (( auto_wmem < 4 )) && auto_wmem=4
          auto_mwmem=$(( mem_total_mb * 5 / 100 ))
          (( auto_mwmem < 64 )) && auto_mwmem=64
          auto_maxconn=$(( 100 + cpu_cnt*50 ))
          wal_keep="${WALKEEP:-256}"

          if [ "${MODE}" = "manual" ]; then
            shared="${SBUFF:-$auto_shared}"
            wmem="${WMEM:-$auto_wmem}"
            mwmem="${MW_MEM:-$auto_mwmem}"
            ecache="${ECACHE:-$auto_ecache}"
            maxconn="${MAXCONN:-$auto_maxconn}"
          else
            shared="${SBUFF:-$auto_shared}"
            wmem="${WMEM:-$auto_wmem}"
            mwmem="${MW_MEM:-$auto_mwmem}"
            ecache="${ECACHE:-$auto_ecache}"
            maxconn="${MAXCONN:-$auto_maxconn}"
          fi

          echoe "DB Profile: shared_buffers=${shared}MB, work_mem=${wmem}MB, maintenance_work_mem=${mwmem}MB, effective_cache_size=${ecache}MB, max_connections=${maxconn}, autovacuum=${AUTOVAC}, wal_keep_size=${wal_keep}MB"
          {
            echo "shared_buffers = '${shared}MB'"
            echo "work_mem = '${wmem}MB'"
            echo "maintenance_work_mem = '${mwmem}MB'"
            echo "effective_cache_size = '${ecache}MB'"
            echo "max_connections = ${maxconn}"
            echo "autovacuum = ${AUTOVAC:-on}"
            echo "wal_keep_size = '${wal_keep}MB'"
            echo "random_page_cost = 1.1"
            echo "effective_io_concurrency = 256"
            echo "max_wal_size = '2GB'"
            echo "min_wal_size = '512MB'"
            echo "checkpoint_completion_target = 0.9"
          } > .github/echo_db/ddl/postgres.tune.conf
          cat .github/echo_db/ddl/postgres.tune.conf | sed 's/^/[ECHO] cfg: /' || true

      - name: DB Parameters ‚Ä¢ apply via ALTER SYSTEM + restart + health
        shell: bash
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          while IFS= read -r line; do
            key=$(echo "$line" | awk -F '=' '{print $1}' | xargs)
            val=$(echo "$line" | cut -d'=' -f2- | xargs)
            [ -z "$key" ] && continue
            run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -c \"alter system set ${key} = ${val};\""
          done < .github/echo_db/ddl/postgres.tune.conf
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -c 'select pg_reload_conf();' || true"
          run "docker restart finops-db"
          retry 40 3 "docker exec finops-db pg_isready -U finops -d finops"
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -c \"select name, setting from pg_settings where name in ('shared_buffers','work_mem','maintenance_work_mem','effective_cache_size','max_connections','autovacuum','wal_keep_size');\""

      - name: Partitions ‚Ä¢ scale horizon by inputs
        shell: bash
        env:
          BACK: ${{ inputs.part_days_back }}
          FWD:  ${{ inputs.part_days_forward }}
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          DB_B="${BACK:-60}"; DB_F="${FWD:-7}"
          echoe "partition horizon: back=${DB_B}d, forward=${DB_F}d"
          cat > .github/echo_db/ddl/003_partitions_scaled.sql <<SQL
          do \$\$
          declare d date := (current_date - interval '${DB_B} days')::date;
                  endd date := (current_date + interval '${DB_F} days')::date;
          begin
            while d <= endd loop
              execute format('create table if not exists transactions_p%s partition of transactions
                              for values from (%L) to (%L);',
                              to_char(d,'YYYYMMDD'), d::timestamptz, (d+1)::timestamptz);
              execute format('create index if not exists ix_txn_%s_acc_ts on transactions_p%s (account_no, ts);',
                              to_char(d,'YYYYMMDD'), to_char(d,'YYYYMMDD'));
              execute format('create table if not exists journal_entries_p%s partition of journal_entries
                              for values from (%L) to (%L);',
                              to_char(d,'YYYYMMDD'), d::timestamptz, (d+1)::timestamptz);
              execute format('create index if not exists ix_je_%s_acc_ts on journal_entries_p%s (account_no, ts);',
                              to_char(d,'YYYYMMDD'), to_char(d,'YYYYMMDD'));
              d := d + 1;
            end loop;
          end\$\$;
          SQL
          run "PGPASSWORD=finops psql -h 127.0.0.1 -U finops -d finops -f .github/echo_db/ddl/003_partitions_scaled.sql"

      - name: PgBouncer ‚Ä¢ optional scale-out connection pooler
        if: ${{ inputs.enable_pgbouncer == 'true' }}
        shell: bash
        env:
          PGB_POOL: ${{ inputs.pgbouncer_pool_size }}
        run: |
          set -Eeuo pipefail
          source /tmp/echo_helpers.sh
          NET="finops_net"
          run "docker rm -f pgbouncer >/dev/null 2>&1 || true"
          run "mkdir -p .github/echo/pgbouncer"
          cat > .github/echo/pgbouncer/pgbouncer.ini <<INI
          [databases]
          finops = host=finops-db port=5432 dbname=finops user=finops password=finops

          [pgbouncer]
          listen_addr = 0.0.0.0
          listen_port = 6432
          auth_type = plain
          auth_file = /etc/pgbouncer/userlist.txt
          pool_mode = transaction
          max_client_conn = 2000
          default_pool_size = ${PGB_POOL:-50}
          server_reset_query = DISCARD ALL
          INI
          echo "\"finops\" \"finops\"" > .github/echo/pgbouncer/userlist.txt
          run "docker run -d --name pgbouncer --network '$NET' -p 6432:6432 \
               -v '$PWD/.github/echo/pgbouncer/pgbouncer.ini':/etc/pgbouncer/pgbouncer.ini \
               -v '$PWD/.github/echo/pgbouncer/userlist.txt':/etc/pgbouncer/userlist.txt \
               edoburu/pgbouncer"
          retry 20 2 "bash -lc '</dev/tcp/127.0.0.1/6432' || exit 1"
          echoe "PgBouncer ready on 6432 (use host=localhost port=6432)"

      # ÏãúÎÆ¨Î†àÏù¥ÌÑ∞ (Kafka ÎòêÎäî COPY Bulk)
      - name: Kafka simulators (if enabled)
        if: ${{ matrix.run_kafka == 'true' }}
        shell: bash
        env:
          ROWS: ${{ matrix.rows }}
        run: |
          echoe "write .github/echo/requirements-extra.txt"
          cat > .github/echo/requirements-extra.txt <<'PIP'
          kafka-python
          psycopg2-binary
          PIP
          echoe "write .github/echo/bin/kafka_producer.py"
          cat > .github/echo/bin/kafka_producer.py <<'PY'
          import os, json, random
          from datetime import datetime, timedelta
          from kafka import KafkaProducer
          ROWS=int(os.getenv("ROWS","2000")); EXTRA=int(os.getenv("EXTRA","150"))
          producer=KafkaProducer(bootstrap_servers="localhost:9092", value_serializer=lambda v: json.dumps(v, default=str).encode())
          accts=[f"V{str(i).zfill(6)}" for i in range(1, 50+EXTRA+1)]
          base=datetime.utcnow()-timedelta(days=3)
          for i in range(ROWS):
            msg={"ts": (base+timedelta(seconds=i*(3*24*3600/ROWS))).isoformat(),
                 "account_no": random.choice(accts),
                 "direction": random.choice(["IN","OUT"]),
                 "amount": round(random.uniform(10, 500000),2),
                 "ref": f"KAFKA{i}"}
            producer.send("transactions", msg)
            if i%500==0: producer.flush()
          producer.flush()
          PY
          echoe "write .github/echo/bin/kafka_consumer.py"
          cat > .github/echo/bin/kafka_consumer.py <<'PY'
          import json, psycopg2
          from kafka import KafkaConsumer
          consumer=KafkaConsumer("transactions", bootstrap_servers="localhost:9092", value_deserializer=lambda m: json.loads(m.decode()))
          conn=psycopg2.connect(host="127.0.0.1", user="finops", password="finops", dbname="finops")
          cur=conn.cursor()
          for msg in consumer:
            r=msg.value
            acc=r["account_no"]; direction=r["direction"]; amount=float(r["amount"]); ts=r["ts"]
            cur.execute("insert into balances(account_no,balance) values(%s,0) on conflict do nothing",(acc,))
            cur.execute("""insert into transactions(account_no,ts,direction,amount,balance_after,ref,note,run_id)
                           values(%s,%s,%s,%s,0,%s,%s,NULL)""",(acc,ts,direction,amount,r.get("ref","kafka"),"kafka"))
            jtype="DEBIT" if direction=="IN" else "CREDIT"
            cur.execute("""insert into journal_entries(account_no,ts,type,amount,memo,run_id)
                           values(%s,%s,%s,%s,%s,NULL)""",(acc,ts,jtype,amount,"kafka"))
            conn.commit()
          PY
          run "python -m pip install --upgrade pip"
          run "pip install -r .github/echo/requirements-extra.txt"
          run "python .github/echo/bin/kafka_consumer.py >/tmp/kafka_consumer.log 2>&1 & echo \$! > /tmp/kafka_consumer.pid"
          run "EXTRA='${{ inputs.extra_accounts }}' ROWS='${{ matrix.rows }}' python .github/echo/bin/kafka_producer.py"
          sleep 5; run "cp /tmp/kafka_consumer.log .github/echo/logs/kafka_consumer.log || true"

      - name: Direct-DB simulator (COPY bulk loader)
        if: ${{ matrix.run_kafka != 'true' }}
        shell: bash
        env:
          ROWS: ${{ matrix.rows }}
          PARQUET: ${{ env.PARQUET }}
          JSONL: ${{ env.JSONL }}
        run: |
          echoe "write .github/echo/requirements.txt"
          cat > .github/echo/requirements.txt <<'PIP'
          psycopg2-binary
          matplotlib
          pyarrow
          PIP
          echoe "write .github/echo/bin/simulate.py"
          cat > .github/echo/bin/simulate.py <<'PY'
          import os, random, csv, io, json
          from datetime import datetime, timedelta
          import psycopg2, psycopg2.extras
          import matplotlib.pyplot as plt
          ROWS=int(os.getenv("ROWS","2000"))
          PARQUET=os.getenv("PARQUET","true").lower()=="true"
          JSONL=os.getenv("JSONL","false").lower()=="true"
          conn=psycopg2.connect(host="127.0.0.1",user="finops",password="finops",dbname="finops")
          cur=conn.cursor()
          cur.execute("select account_no from accounts order by account_no;")
          accts=[r[0] for r in cur.fetchall()]
          base=datetime.utcnow()-timedelta(days=10)
          txn_csv=io.StringIO(); je_csv=io.StringIO()
          txn_writer=csv.writer(txn_csv); je_writer=csv.writer(je_csv)
          rows=[]
          for i in range(ROWS):
            acc=random.choice(accts)
            ts=base+timedelta(seconds=i*(10*24*3600/ROWS))
            direction=random.choice(["IN","OUT"])
            amount=round(random.uniform(10, 800000),2)
            txn_writer.writerow([acc,ts.isoformat(),direction,amount,0,f"REF{i}","sim",None])
            jtype="DEBIT" if direction=="IN" else "CREDIT"
            je_writer.writerow([acc,ts.isoformat(),jtype,amount,"sim",None])
            rows.append({"ts":ts,"account_no":acc,"direction":direction,"amount":amount,"balance_after":0.0})
          txn_csv.seek(0); je_csv.seek(0)
          psycopg2.extras.execute_values(cur,"insert into balances(account_no,balance) values %s on conflict do nothing",[(r["account_no"],0) for r in rows])
          cur.copy_expert("""COPY transactions(account_no,ts,direction,amount,balance_after,ref,note,run_id) FROM STDIN WITH (FORMAT csv)""", txn_csv)
          cur.copy_expert("""COPY journal_entries(account_no,ts,type,amount,memo,run_id) FROM STDIN WITH (FORMAT csv)""", je_csv)
          conn.commit()
          # enrich balances
          cur.execute("""
            with ch as (
              select account_no, sum(case when direction='IN' then amount else -amount end) as net
              from transactions where run_id is null group by account_no
            )
            insert into balances(account_no,balance)
            select account_no, net from ch
            on conflict (account_no) do update set balance = balances.balance + excluded.balance;
          """); conn.commit()
          # charts
          cur.execute("""
            select date_trunc('day', ts) d,
                   sum(case when direction='IN' then amount else 0 end) rev,
                   sum(case when direction='OUT' then amount else 0 end) exp,
                   sum(case when direction='IN' then amount else -amount end) prof
            from transactions group by 1 order by 1
          """)
          D=[(r[0],float(r[1] or 0),float(r[2] or 0),float(r[3] or 0)) for r in cur.fetchall()]
          dates=[r[0] for r in D]; rev=[r[1] for r in D]; exp=[r[2] for r in D]; prof=[r[3] for r in D]
          import os
          os.makedirs("site/imgs",exist_ok=True)
          plt.figure(); plt.plot(dates,rev,label="Revenue"); plt.plot(dates,exp,label="Expense"); plt.plot(dates,prof,label="Profit"); plt.legend(); plt.tight_layout()
          plt.savefig("site/imgs/income_statement.png")
          cur.execute("select sum(balance) from balances;"); assets=float(cur.fetchone()[0] or 0)
          plt.figure(); plt.bar(["Assets"],[assets]); plt.tight_layout(); plt.savefig("site/imgs/balance_sheet.png")
          os.makedirs("site/reports",exist_ok=True)
          # CSV
          with open("site/reports/transactions.csv","w",newline="",encoding="utf-8") as f:
            w=csv.writer(f); w.writerow(["ts","account_no","direction","amount","balance_after"])
            for r in rows: w.writerow([r["ts"],r["account_no"],r["direction"],r["amount"],r["balance_after"]])
          # Parquet/JSONL (ÏòµÏÖò)
          if PARQUET:
            try:
              import pyarrow as pa, pyarrow.parquet as pq
              table=pa.Table.from_pylist(rows); pq.write_table(table,"site/reports/transactions.parquet")
            except Exception as e:
              print("parquet write skipped:", e)
          if JSONL:
            with open("site/reports/transactions.jsonl","w",encoding="utf-8") as f:
              for r in rows:
                r2 = dict(r); r2["ts"]=r2["ts"].isoformat(); f.write(json.dumps(r2,ensure_ascii=False)+"\n")
          PY
          run "python -m pip install --upgrade pip"
          run "pip install -r .github/echo/requirements.txt"
          run "ROWS='${{ matrix.rows }}' PARQUET='${PARQUET}' JSONL='${JSONL}' python .github/echo/bin/simulate.py"

      # Í≤ÄÏ¶ù(Í∞ïÌôî) + Step Summary
      - name: Verify Simulation Writes (strong checks)
        shell: bash
        env:
          TARGET_ROWS: ${{ matrix.rows }}
        run: |
          cat > .github/echo/bin/verify.py <<'PY'
          import os, json, psycopg2
          tgt=int(os.getenv("TARGET_ROWS","0"))
          conn=psycopg2.connect(host="127.0.0.1", user="finops", password="finops", dbname="finops")
          cur=conn.cursor()
          cur.execute("select count(*) from transactions"); c_txn=cur.fetchone()[0]
          cur.execute("select count(*) from journal_entries"); c_je=cur.fetchone()[0]
          cur.execute("select min(ts), max(ts) from transactions"); tmin,tmax=cur.fetchone()
          ok = (c_txn>=tgt and c_je>=tgt and tmin is not None and tmax is not None)
          report={"target":tgt,"txn_count":int(c_txn),"journal_count":int(c_je),"time_range":[str(tmin),str(tmax)],"ok":ok}
          os.makedirs("site/reports",exist_ok=True)
          open("site/reports/write_verify.json","w").write(json.dumps(report,indent=2))
          cur.close(); conn.close()
          import sys
          if not ok: sys.exit("Verification failed: counts/time_range invalid")
          PY
          run "python .github/echo/bin/verify.py"
          echo "| Key | Value |" >> "$GITHUB_STEP_SUMMARY"
          echo "|---|---|" >> "$GITHUB_STEP_SUMMARY"
          echo "| Rows | ${{ matrix.rows }} |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Kafka | ${{ matrix.run_kafka }} |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Encrypt | ${{ matrix.encrypt_outputs }} |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Reports dir | \`artifacts/reports\`, \`site/reports\` |" >> "$GITHUB_STEP_SUMMARY"

      - name: MinIO (mc) mirror outputs
        shell: bash
        run: |
          retry 5 3 "curl -fsSL https://dl.min.io/client/mc/release/linux-amd64/mc -o mc"
          run "chmod +x mc"
          retry 20 2 "./mc alias set local http://127.0.0.1:9000 admin admin12345"
          run "./mc mb --ignore-existing local/finops"
          run "./mc mirror --overwrite site local/finops/site"
          run "./mc mirror --overwrite .github/echo local/finops/echo"
          run "./mc ls -r local/finops | tee .github/echo/logs/minio.tree"

      - name: DB Backup (pg_dumpall)
        shell: bash
        run: |
          run "PGPASSWORD=finops pg_dumpall -h 127.0.0.1 -U finops > .github/echo_db/backup/pg_dumpall.sql"
          run "gzip -c .github/echo_db/backup/pg_dumpall.sql > artifacts/db/pg_dumpall.sql.gz"

      - name: Package outputs + Checksums
        shell: bash
        run: |
          run "mkdir -p artifacts/zip artifacts/checks"
          run "cp -a .github/echo_db/ddl site/sql/"
          run "cp -a .github/echo_db/seed site/sql/ || true"
          run "cp -a .github/echo_db/views site/sql/"
          run "cp -a site/imgs artifacts/imgs/"
          run "cp -a site/reports artifacts/reports/"
          run "cp -a site/sql artifacts/sql/"
          run "cp -a .github/echo/logs artifacts/logs/"
          run "cp -a .github/echo_db/backup artifacts/db/"
          run "(cd artifacts && zip -r zip/finops_outputs.zip imgs reports sql logs db >/dev/null)"
          run "(cd artifacts/zip && sha256sum finops_outputs.zip | tee ../checks/finops_outputs.zip.sha256)"
          run "(cd artifacts/zip && sha512sum finops_outputs.zip | tee ../checks/finops_outputs.zip.sha512)"

      - name: SBOM (Syft) + Vuln (Grype)
        shell: bash
        run: |
          SYFT_VER="v1.31.0"
          if ! curl -fsSL https://raw.githubusercontent.com/anchore/syft/main/install.sh | bash -s -- -b /usr/local/bin "$SYFT_VER"; then
            run "curl -fsSL https://raw.githubusercontent.com/anchore/syft/${SYFT_VER}/install.sh | bash -s -- -b /usr/local/bin ${SYFT_VER}"
          fi
          run "syft version"
          run "syft dir:. -o spdx-json > artifacts/sbom/repo.spdx.json"
          if curl -fsSL https://raw.githubusercontent.com/anchore/grype/main/install.sh | bash -s -- -b /usr/local/bin; then
            run "grype version"
            run "grype dir:. -o sarif > artifacts/scan/grype.sarif || true"
          else
            warn "Grype install failed ‚Äì skipping grype scan"
          fi

      - name: Security Scan (Trivy + Gitleaks, auto flags)
        shell: bash
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          run "mkdir -p artifacts/scan"
          run "curl -fsSL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin"
          run "trivy fs --scanners vuln,misconfig,secret --format json --output artifacts/scan/trivy.json ."
          API="https://api.github.com/repos/gitleaks/gitleaks/releases/latest"
          ASSET_URL="$(curl -fsSL -H "Authorization: Bearer ${GH_TOKEN}" "$API" | jq -r '.assets[] | select(.name | test("linux_x64.*\\.tar\\.gz$")) | .browser_download_url' | head -n1)"
          [ -z "$ASSET_URL" -o "$ASSET_URL" = "null" ] && ASSET_URL="https://github.com/gitleaks/gitleaks/releases/download/v8.28.0/gitleaks_8.28.0_linux_x64.tar.gz"
          echoe "Gitleaks asset: $ASSET_URL"
          run "curl -fsSL \"$ASSET_URL\" -o /tmp/gitleaks.tgz"
          if ! tar -xzf /tmp/gitleaks.tgz -C /usr/local/bin gitleaks; then
            run "tar -xzf /tmp/gitleaks.tgz -C /tmp && mv /tmp/gitleaks*/gitleaks /usr/local/bin/"
          fi
          run "chmod +x /usr/local/bin/gitleaks"
          run "gitleaks version || true"
          if gitleaks detect --help 2>/dev/null | grep -q -- '--report-path'; then
            echoe "gitleaks: using --report-path/--report-format"
            run "gitleaks detect --redact --no-git -v --report-format sarif --report-path artifacts/scan/gitleaks.sarif || true"
          else
            echoe "gitleaks: using legacy -o/--report-format"
            run "gitleaks detect --redact --no-git -v --report-format sarif -o artifacts/scan/gitleaks.sarif || true"
          fi

      - name: Encrypt outputs (optional AES-256)
        if: ${{ matrix.encrypt_outputs == 'true' }}
        shell: bash
        env:
          ENC_PASS_OPT: ${{ env.ENC_PASS_OPT }}
        run: |
          PASS="${{ inputs.enc_pass }}"
          [ -z "$PASS" ] && PASS="${ENC_PASS_OPT:-}"
          [ -z "$PASS" ] && PASS="$(openssl rand -hex 16)"
          echoe "write .github/echo/crypto/enc.pass"
          printf "%s\n" "enc_pass=$PASS" > .github/echo/crypto/enc.pass
          run "openssl enc -aes-256-cbc -md sha256 -salt -pbkdf2 -in artifacts/zip/finops_outputs.zip -out artifacts/zip/finops_outputs.zip.enc -k '$PASS'"
          run "(cd artifacts/zip && sha256sum finops_outputs.zip.enc | tee ../checks/finops_outputs.zip.enc.sha256)"
          run "(cd artifacts/zip && sha512sum finops_outputs.zip.enc | tee ../checks/finops_outputs.zip.enc.sha512)"

      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: finops-echoops-v810
          path: |
            artifacts/**
            site/**
            .github/echo/**
          if-no-files-found: warn
          retention-days: 14

      # Cosign keyless ÏÑúÎ™Ö & Í∞ÑÎã® provenance (ÏÑ†ÌÉù, Ïã§Ìå® Î¨¥Ïãú)
      - name: Cosign sign ZIP (keyless) + provenance
        if: ${{ inputs.create_release == 'true' }}
        shell: bash
        env:
          COSIGN_EXPERIMENTAL: "1"
        run: |
          curl -fsSL https://github.com/sigstore/cosign/releases/latest/download/cosign-linux-amd64 -o /usr/local/bin/cosign && chmod +x /usr/local/bin/cosign || { warn "cosign install failed"; exit 0; }
          cat > artifacts/checks/provenance.json <<'JSON'
          {
            "schema":"io.echoops.provenance/v1",
            "subject":"artifacts/zip/finops_outputs.zip",
            "generator":"echoops-xl",
            "inputs":["rows","run_kafka","encrypt_outputs","extra_accounts","options_json"]
          }
          JSON
          for f in artifacts/zip/finops_outputs.zip artifacts/zip/finops_outputs.zip.enc artifacts/checks/*.sha256 artifacts/checks/*.sha512 artifacts/checks/provenance.json; do
            [ -f "$f" ] || continue
            echoe "cosign sign $f"
            cosign sign-blob --yes --output-signature "$f.sig" "$f" || warn "cosign failed for $f"
          done

      - name: Commit & Push (safe refspec)
        if: ${{ inputs.commit_changes == 'true' }}
        shell: bash
        env:
          GIT_AUTHOR_NAME: "github-actions[bot]"
          GIT_AUTHOR_EMAIL: "41898282+github-actions[bot]@users.noreply.github.com"
          GIT_COMMITTER_NAME: "github-actions[bot]"
          GIT_COMMITTER_EMAIL: "41898282+github-actions[bot]@users.noreply.github.com"
        run: |
          BRANCH="${GITHUB_REF_NAME:-main}"
          run "git config user.name  \"$GIT_AUTHOR_NAME\""
          run "git config user.email \"$GIT_AUTHOR_EMAIL\""
          run "git add -A"
          if git diff --cached --quiet; then echoe "Î≥ÄÍ≤Ω ÏóÜÏùå ‚Üí Ìë∏Ïãú Ïä§ÌÇµ"; exit 0; fi
          run "git commit -m \"[ECHO] v8.1.0 XL (health+summary+sbom/grype+cosign+checksums+dbtune+pgbouncer+partitions+copy+formats+verify), rows=${{ matrix.rows }}, kafka=${{ matrix.run_kafka }}, enc=${{ matrix.encrypt_outputs }}\""
          run "git fetch --tags --force origin"
          run "git push origin \"refs/heads/${BRANCH}:refs/heads/${BRANCH}\" --force-with-lease"

      - name: Create Release (optional)
        if: ${{ inputs.create_release == 'true' }}
        uses: softprops/action-gh-release@v2
        with:
          tag_name: finops-echoops-v810-${{ github.run_number }}
          name: "FinOps EchoOps v8.1.0 ‚Ä¢ XL ‚Ä¢ Run #${{ github.run_number }}"
          draft: false
          prerelease: false
          files: |
            artifacts/zip/finops_outputs.zip
            artifacts/zip/finops_outputs.zip.enc
            artifacts/checks/*
            artifacts/scan/*
            artifacts/sbom/*

      # üßπ Ï¢ÖÎ£å ÌõÖ(Post): Ìï≠ÏÉÅ Ïã§Ìñâ ‚Äì ÏÉÅÌÉú/Î°úÍ∑∏/Î¶¨ÏÜåÏä§ Ïä§ÎÉÖÏÉ∑ ÏàòÏßë
      - name: Post-Hook ‚Ä¢ collect diagnostics (always)
        if: ${{ always() }}
        shell: bash
        run: |
          mkdir -p artifacts/logs/post
          (docker ps -a || true) > artifacts/logs/post/docker.ps.txt
          for c in finops-db finops-adminer finops-minio prometheus grafana loki promtail cadvisor node-exporter finops-zk finops-kafka otel pgbouncer; do
            docker logs --tail=200 "$c" > "artifacts/logs/post/${c}.log" 2>&1 || true
          done
          (df -h || true) > artifacts/logs/post/df_h.txt
          (free -m || true) > artifacts/logs/post/free_m.txt
          (uptime || true) > artifacts/logs/post/uptime.txt
